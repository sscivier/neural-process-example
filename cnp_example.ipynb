{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import equinox as eqx\n",
    "import optax\n",
    "\n",
    "import numpyro.distributions as dist\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "from typing import List, Tuple\n",
    "from jaxtyping import Int, Array, Float, PyTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the MLP model, to be used as a batch MLP\n",
    "# Inherit the eqx.Module class\n",
    "\n",
    "class MLP(eqx.Module):\n",
    "    \n",
    "    layers: List\n",
    "        \n",
    "    def __init__(\n",
    "        self, \n",
    "        layer_sizes: List, \n",
    "        key: jax.random.PRNGKey,\n",
    "    ):\n",
    "        \n",
    "        self.layers = []\n",
    "        \n",
    "        for (feat_in, feat_out) in zip(layer_sizes[:-2], layer_sizes[1:-1]):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            \n",
    "            self.layers.append(\n",
    "                eqx.nn.Linear(feat_in, feat_out, use_bias=True, key=subkey)\n",
    "            )  # fully-connected layer\n",
    "            self.layers.append(\n",
    "                jnp.tanh\n",
    "            )  # activation function\n",
    "        \n",
    "        key, subkey = jax.random.split(key)\n",
    "        \n",
    "        self.layers.append(\n",
    "            eqx.nn.Linear(layer_sizes[-2], layer_sizes[-1], use_bias=True, key=subkey)\n",
    "        )  # final layer\n",
    "    \n",
    "    # __call__ turns an instance of this class into a callable object, which behaves like a function\n",
    "    def __call__(\n",
    "        self, \n",
    "        x: Float[Array, \"1 1\"],\n",
    "    ) -> Float[Array, \"1 1\"]:\n",
    "        \n",
    "        # apply each layer in sequence\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNP(eqx.Module):\n",
    "\n",
    "    layers: list\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder: MLP,\n",
    "            decoder: MLP,\n",
    "    ):\n",
    "        \n",
    "        self.layers = [encoder, decoder]\n",
    "\n",
    "\n",
    "    def __call__(\n",
    "            self,\n",
    "            x_context: Float[Array, \"batch n_context 1\"],\n",
    "            y_context: Float[Array, \"batch n_context 1\"],\n",
    "            x_target: Float[Array, \"batch n_target 1\"],\n",
    "    ) -> dist.Distribution:\n",
    "\n",
    "        # get number of target points\n",
    "        _, n_target, _ = x_target.shape\n",
    "\n",
    "        # encoder step\n",
    "        encoded_rep = self._encode(x_context, y_context)  # (batch_size, 1, encoder_dim)\n",
    "\n",
    "        # tile sample before passing to the decoder\n",
    "        representation = self._tile(encoded_rep, n_target)  # (batch_size, n_target, encoder_dim)\n",
    "\n",
    "        # decoder step to produce distribution of functions\n",
    "        distribution = self._decode(representation, x_target)\n",
    "\n",
    "        return distribution\n",
    "    \n",
    "\n",
    "    def _encode(\n",
    "            self,\n",
    "            x_context: Float[Array, \"batch n_context 1\"], \n",
    "            y_context: Float[Array, \"batch n_context 1\"],\n",
    "    ) -> dist.Distribution:\n",
    "        \n",
    "        xy_context = jnp.concatenate([x_context, y_context], axis=-1)  # (batch_size, n_context, 2)\n",
    "        \n",
    "        return jnp.mean(self._encode_mlp(xy_context), axis=1, keepdims=True)  # (batch_size, 1, encoder_dim)\n",
    "    \n",
    "\n",
    "    def _encode_mlp(\n",
    "            self,\n",
    "            xy_context: Float[Array, \"batch n_context 2\"],\n",
    "    ) -> Float[Array, \"batch n_context encoder_dim\"]:\n",
    "        \n",
    "        return jax.vmap(jax.vmap(self.layers[0]))(xy_context)\n",
    "    \n",
    "\n",
    "    def _tile(\n",
    "            self,\n",
    "            z_latent: Float[Array, \"batch 1 encoder_dim\"],\n",
    "            n_target: Int,\n",
    "    ) -> Float[Array, \"batch n_target encoder_dim\"]:     \n",
    "\n",
    "        return jnp.tile(z_latent, [1, n_target, 1])\n",
    "    \n",
    "\n",
    "    def _decode(\n",
    "            self,\n",
    "            representation: Float[Array, \"batch n_target encoder_dim\"],\n",
    "            x_target: Float[Array, \"batch n_target 1\"],\n",
    "    ) -> dist.Distribution:\n",
    "        \n",
    "        representation = jnp.concatenate([representation, x_target], axis=-1)  # (batch_size, n_target, encoder_dim + 1)\n",
    "\n",
    "        mlp_out = jax.vmap(jax.vmap(self.layers[1]))(representation)  # (batch_size, n_target, 1)\n",
    "\n",
    "        mu, sigma = jnp.split(mlp_out, 2, axis=-1)  # each (batch_size, n_target, 1)\n",
    "\n",
    "        sigma = 0.1 + 0.9 * jax.nn.softplus(sigma)\n",
    "        \n",
    "        return dist.Normal(loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(\n",
    "        model: CNP,\n",
    "        x_context: Float[Array, \"batch n_context 1\"],\n",
    "        y_context: Float[Array, \"batch n_context 1\"],\n",
    "        x_target: Float[Array, \"batch n_context 1\"],\n",
    "        y_target: Float[Array, \"batch n_context 1\"],\n",
    ") -> Float[Array, \"\"]:\n",
    "    \n",
    "    distribution = model(x_context, y_context, x_target, y_target)\n",
    "\n",
    "    return np.mean(distribution.log_prob(y_target))\n",
    "\n",
    "loss_and_grad = eqx.filter_value_and_grad(loss)\n",
    "\n",
    "loss = eqx.filter_jit(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "        model: CNP,\n",
    "        test_loader: DataLoader,\n",
    ") -> Float[Array, \"\"]:\n",
    "    \n",
    "    tot_loss = 0\n",
    "\n",
    "    for (x_context, y_context, x_target, y_target) in test_loader:\n",
    "        x_context, y_context, x_target, y_target = x_context.numpy(), y_context.numpy(), x_target.numpy(), y_target.numpy()\n",
    "        tot_loss += loss(model, x_context, y_context, x_target, y_target)\n",
    "\n",
    "    return tot_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: CNP,\n",
    "        train_loader: DataLoader,\n",
    "        test_loader: DataLoader,\n",
    "        optim: optax.GradientTransformation,\n",
    "        epochs: Int,\n",
    "        plot_every: Int,\n",
    ") -> CNP:\n",
    "    \n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(\n",
    "        model: CNP,\n",
    "        opt_state: PyTree,\n",
    "        x_context: Float[Array, \"batch n_context 1\"],\n",
    "        y_context: Float[Array, \"batch n_context 1\"],\n",
    "        x_target: Float[Array, \"batch n_target 1\"],\n",
    "        y_target: Float[Array, \"batch n_target 1\"],\n",
    "    ) -> Tuple[CNP, PyTree, Float]:\n",
    "        \n",
    "        loss_value, grads = loss_and_grad(\n",
    "            model,\n",
    "            x_context,\n",
    "            y_context,\n",
    "            x_target,\n",
    "            y_target,\n",
    "        )\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "\n",
    "        return model, opt_state, loss_value\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    # Create a function to update the loss plot\n",
    "    def update_loss_plot(train_losses, test_losses):\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        plt.plot(train_losses, 'o-', markevery=[-1], label='train_loss')\n",
    "        plt.plot(test_losses, 'o-', markevery=[-1], label='test_loss')\n",
    "        plt.xlim([0, epochs])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for (x_context, y_context, x_target, y_target) in train_loader:\n",
    "            x_context, y_context, x_target, y_target = x_context.numpy(), y_context.numpy(), x_target.numpy(), y_target.numpy()\n",
    "\n",
    "            model, opt_state, train_loss = make_step(\n",
    "                model,\n",
    "                opt_state,\n",
    "                x_context,\n",
    "                y_context,\n",
    "                x_target,\n",
    "                y_target,\n",
    "            )\n",
    "            epoch_loss += train_loss.item()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        if (epoch % plot_every) == 0:\n",
    "            test_loss = evaluate(model, test_loader)\n",
    "            train_losses.append(avg_epoch_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            update_loss_plot(train_losses, test_losses)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
